# SLRKIT command
The `slrkit` command helps to handle a slr-kit project.
This command automates and handles all the phases of the document analysis.
To do so, a set of configuration files are used in order to automate the process.
These files are stored in a slr-kit project and are created and managed by the `slrkit` command itself.

## SLR-KIT projects

An SLR-KIT project is a collection of files generated by the SLR-KIT scripts.
All of these files, are generated from a set of documents that the user wishes to analyze.
A project is also a `git` repository.
The `slrkit` command initializes this repository when the project is first created and helps to track only the meaningful files.

### Anathomy of a project
An SLR-KIT project is a directory that contains all the files related to an analysis.
This directory must contain also a `META.toml` file and a project configuration directory.

#### META.toml
This file contains all the metadata about the project.
It must be a [TOML version 1.0.0](https://toml.io/en/v1.0.0) file.
It must contain two dictionaries: `Project` and `Source`.

##### `Project` dictionary
This contains information about the project such as the name of the project, a description, the location of the configuration directory and some more.
The allowed keys and their meaning are described in the following table:

| Key         | Description                                 | Type            |
|-------------|---------------------------------------------|:---------------:|
| Author      | Information about the author of the project |      string     |
| Config      | Name of the configuration directory         |      string     |
| Description | Description of the project                  |      string     |
| Keywords    | List of keywords related to the documents   | list of strings |
| Name        | Name of the project                         |      string     |

##### `Source` dictionary
This contains information about the source of the documents analyzed in the project.
The allowed keys and their meaning are described in the following table:

| Key    | Description                                    | Type   |
|--------|------------------------------------------------|:------:|
| URL    | URL of the site used to retrieve the documents | string |
| Query  | Query string usedto retrieve the documents     | string |
| Date   | Date on which the documents were retrieved     | string |
| Origin | Description of the origin of the documents     | string |

The `Origin` key is meant to be used when the documents are retrieved without the help of a bibligraphical search engine.
In this case the `URL` and `Query` keys shall be left empty.

#### The project configuration directory
This directory is located inside the project directory.
Its name is saved in the `META.toml` file in the `Project.Config` key.
The default name for this directory is `slrkit.conf` but a different name may be used.
This directory contains all the configuration files used by the project.
The configuration files must be TOML v. 1.0.0 files.
Information about each file can be found in the documentation of each `slrkit` subcommand.
Each relative path included in the configuration files are considered to be relative to the project root directory.
The configuration directory contains also a `log` directory that contains all the log files produced during the project.
All the scripts that write a log use the `slr-kit.log` log file saved in the log directory.

## The `slrkit` command
The `slrkit` command is the tool to handle a project.
It uses the `META.toml` and the file in the project configuration directory to automate the operations.
It is composed by some sub-commands to handle and automate all the phases of a project.

Usage:

    python3 slrkit.py [-C /path/to/project] sub-command sub-command-arguments ...

The sub-commands are:
* [`init`](#init): initialize a slr-kit project
* [`import`](#import): import a bibliographic database converting to the csv format used by slr-kit.
* [`journals`](#journals): subcommand to extract and filter a list of journals. Requires a subcommand.
* [`acronyms`](#acronyms): extract acronyms from texts.
* [`preprocess`](#preprocess): run the preprocess stage in a slr-kit project
* [`terms`](#terms): subcommand to extract and handle lists of terms in a slr-kit project. Requires a sub-command
* [`fawoc`](#fawoc): run fawoc in a slr-kit project.
* [`lda`](#lda): run the lda stage in a slr-kit project
* [`report`](#report): run the report creation script in a slr-kit project.
* [`optimize_lda`](#optimize_lda): run an optimization phase for the lda stage in aslr-kit project, using a GA.
* [`record`](#record): record a snapshot of the project in the underlying git repository
* [`lda_grid_search`](#lda_grid_search): run an optimization phase for the lda stage in a slr-kit project using a grid search method.

Each command operates on the directory from which the `slrkit` command is run.
The `-C` option allows to change the current directory to the one specified.

### init
Initialize the current directory as an SLR-KIT project.
Usage:

    python3 slrkit.py init [--author AUTHOR] [--description DESCRIPTION] [--no-backup] name

Argument `name` is the name of the project.
It will be used as a prefix for all the suggested file names.
The `--author` option allows to specify the project author while the `--description` option allows to specify the project description.
It creates the `META.toml` files with information from the command line.
**The user shall complete the content of this file**.

This command also creates the configuration directory.
This directory is populated with all the configuration files handled by the `slrkit` command.
The file format is [TOML version 1.0.0](https://toml.io/en/v1.0.0).
The name of each file is the name of the `slrkit` sub-command (e.g. `preprocess.toml` is the configuration file for the `slrkit preprocess` command) and it contains a key for each parameter of the corresponding script.
Refer to the documentation of each script and command for additional information about the configuration parameters.
In each file some comments explain each parameter, and the output file name of each script are suggested with some good default names.

The `init` command also copies the `ga_param.toml` file to the configuration directory with the name `optimize_lda_ga_params.toml`.
This file is used by the `optimize_lda` command for the parameters used in the optimization.
See the documentation of the `optimize_lda` command for more information.

This command can be executed on an already initialized project.
In this case the information in the `META.toml` are updated with the ones given on the command line.
All the other fields are left untouched.
The configuration files are updated.
If one or more option are missing, they are filled with the default value.
The other information are not changed.
The original `toml` files are backuped in the configuration directory before any modification.
The backups have the same name of the original files with the extension `.bak`.
If the user gives the `--no-backup` option, no backup is performed.

The `init` sub-command also initializes the `git` repository of the project.
A `.gitignore` file is provvided with some useful patterns.
A first commit is recorded with:

* the `META.toml` file;
* all the configuration files;
* the provvided `.gitignore`.

### import
This command imports a bibliographical database into the project, converting it to the `Â¢sv` format used by all the scripts.
The output of this command will be called the *abstracts* file in the rest of this document.

Usage:

    python3 slrkit.py import [--list_columns]

The `import` sub-command uses the `import.toml` configuration file and runs the `import_biblio.py` script.
It imports the database in a `csv` usable by the other commands.
To each paper is assigned a progressive identification number in the column `id`.
All the selected columns are imported from the input file.
The citation count for each paper is also retrived and imported as the column `citation`.
If the option `--list_columns` is set, the command outputs only the list of available columns of the input file specified in the configuration file and no data is imported.

After a correct execution, the command changes `journals_extract.toml`, `journals_filter.toml` and `report.toml` files updating their `ris_file` field with the name of the input file.

The `import.toml` has the following structure:

* `input_file`: path to the bibliographical database to import. **Important:** this field is not pre-filled by the `init` command, the user **must** fill it before running the `import` command. This file is committed to `git` repository by the `record` command;
* `type`: type of the database to import. Actually the only supported type is `RIS`;
* `output`: name of the output file. It is pre-filled with `<project-name>_abstracts.csv`;
* `columns`: comma separed list of columns to import. It is pre-filled with `title,abstract,year`. Additionally, a `citation` column is also imported by default.

### journals
This command allows the user to retrieve a list of journals and classify them in order to filter out the not relevant ones and the papers published on them.

This command accepts two sub-commands:

* `extract`: extracts the list of journals from the bibliographical database;
* `filter`: uses the manual classification of the list of journals to filter out the papers published on the not relevant journals.

Usage:

    python3 slrkit.py journals {extract, filter}

If the `journals` command is invoked without a sub-command, the `extract` sub-command is run.

#### journals extract
The `extract` sub-command produces a list in the format used by `FAWOC`.
The structure is the following:

* `id`: a progressive identification number;
* `term`: the name of the journal;
* `label`: the label added by `FAWOC` to the journal. This field is left blank by the `extract` sub-command;
* `count`: the number of papers pubblished in the journal.

`FAWOC` will move the count field in the `fawoc_data.tsv` file.

The `extract` sub-command uses the `journals_extract.toml` configuration file and runs the `journal_lister.py` script.
The `journals_extract.toml` file has the following structure:

* `ris_file`: name of the bibliographical database. It is pre-filled with `<project-name>.ris` and is updated by the `import` command;
* `outfile`: name of the output file. It is pre-filled with `<project-name>_journals.csv`.


#### journals filter
The `filter` sub-command filters the papers using the manual classification of the list of journals.
It adds the `status` column to the *abstracts* file.
This column will have the value `good` for the papers published in a journal classified with `relevant` or the `keyword` label.
All the papers from journals not classified as `relevant` or `keyword` will be marked with the `rejected` value in the `status` column.

The `filter` sub-command uses the `journals_filter.toml` configuration file and runs the `filter_paper.py` script.
The `journals_filter.toml` file has the following structure:

* `ris_file`: name of the bibliographical database. It is pre-filled with `<project-name>.ris` and is updated by the `import` command;
* `abstract_file`: name of the *abstract* file. It is pre-filled with `<project-name>_abstracts.csv`;
* `journal_file`: name of the journal list file produced by `journal extract`. It is pre-filled with `<project-name>_journals.csv`.

### acronyms
This commands extracts acronyms from the papers.
Its output format is suitable to be used with `FAWOC` to classify which acronym is relevant or not.
If the input file (the *abstract* file) contains the `status` column created by the `journals filter` command, the `acronyms` command uses that column value to filter out the paper pubblished in the rejected journals.
The output of this command will be called the *acronyms* file in the rest of this document.

Usage:

    python3 slrkit.py acronyms

The `acronyms` sub-command uses the `acronyms.toml` configuration file and runs the `acronyms.py` script.
The output is in `tsv` format and has the following structure (suitable for `FAWOC`):

* `id`: a progressive identification number;
* `term`: the acronym in the form `extended-acronym | (abbreviation)`;
* `label`: the label added by `FAWOC` to the acronym. This field is left blank by the `acronyms` command.

No `fawoc_data` file is produced, so no `count` field is available for `FAWOC`.

After a correct execution, the command changes the `preprocess.toml` file updating its `acronyms` field with the name of the output file.
All the commands consider only the acronyms classified with the `relevant` or the `keyword` label.
All the other acronyms are not considered.

The `acronyms.toml` has the following structure:

* `datafile`: input file. It is pre-filled with the value `<project-name>_abstracts.csv`;
* `output`: output file. It is pre-filled with the value `<project-name>_acronyms.csv`;
* `columns`: name of the column of `datafile` with the text to elaborate. It is prefilled with the value `abstract`.


### preprocess
The `preprocess` sub-command prepares the documents for the following elaborations.

Usage:

    python3 slrkit.py preprocess

If the input file (the *abstract* file) contains the `status` column created by the `journals filter` command, the `preprocess` command uses that column value to filter out the paper pubblished in the rejected journals.
It also filters the stop-words using the list of words providded by the user.
No default list of stop-words is used, the user **must** provvide his own lists.

This command also uses the *acronyms* file to search and mark the acronyms as relevant words.
Only the acronyms with the `relevant` or the `keyword` label are considered.

The `preprocess` command, also mark as relevant all the terms provvided by the user in the relevant terms lists.
The user can also choose how the command marks this terms.
The input of this command is the *abstract* file.
The output of this command is the *abstract* file without the paper discarded because pubblished in rejected journals.
To this file is also added a new column with the text of each paper preprocessed.
More information can be found in the preprocess.py section of the [README.md](README.md) 

The `preprocess` sub-command uses the `preprocess.toml` configuration file.
This file has the following structure:

* `datafile`: the name of the *abstract* file that will be used as input. This field is pre-filled with `<project-name>_abstracts.csv`;
* `output`: output file name. This field is pre-filled with `<project-name>_preproc.csv`;
* `placeholder`: placeholder used to mark the barriers (the stop-words and the punctuaction). This character is also used as prefix and suffix for the placeholder for the relevant terms and the acronyms. It is prefilled with the character `@`;
* `stop-words`: lists of stop-words provvided by the user. No other lists are used, so the user shall provvide its own;
* `relevant-term`: lists of relevant terms. This field is particular. Each element must be a list of at least one item and at most two items. The first item is the name of a list of relevant terms. The second one, if present, is the marker that the user want to be used for all the terms in this list. All the terms will be marked with `<placeholder><marker><placeholder>`. If the marker is ommitted than the command replaces every term using the placeholder, all the words of the term separed with `_` character and then another placeholder;
* `acronyms`: name of the *acronyms* file. If the `acronyms` command is run before, it is pre-filled with `<project-name>_acronyms.csv`;
* `target-column`: name of the column used for the document text. It is pre-filled with `abstract`;
* `output-column`: name of the column that is added to the output, containing the preprocessed text. It is pre-filled with `abstract_lem`;
* `input-delimiter`: input file field delimiter. It is pre-filled with `\t`;
* `output-delimiter`: input file field delimiter. It is pre-filled with `\t`;
* `rows`: number of rows of the input file to process. If empty, all the rows are used;
* `language`: language of text. Must be a ISO 639-1 two-letter code. Pre-filled with `en`;
* `regex`: csv file with some dataset specific regex substitutions that has to be applied to the text.

The output of this command will be called the *preprocess* file in the rest of this document.

### terms
This command allows the user to generate and handle lists of terms.

This command accepts one sub-command:

* `generate`: generate a list of terms that have to be classified.

Usage:

    python3 slrkit.py terms {generate}

If the `terms` command is invoked without a sub-command, the `generate` sub-command is run.

#### terms generate
The `generate` sub-command generates a list of terms from the documents in the *preprocess* file.
This command runs the `gen_terms.py` script.
The format of this list is the one used by `FAWOC`. The structure is the following:

* `id`: a progressive identification number;
* `term`: the n-gram;
* `label`: the label added by `FAWOC` to the n-gram. This field is left blank by the `terms generate` command.

This command produces also the `fawoc_data.tsv` file, with the following structure:

* `id`: the identification number of the term;
* `term`: the term;
* `count`: the number of occurrences of the term.

The output of this command will be called the *terms* file in the rest of this document.

The `terms generate` sub-command uses the `terms_generate.toml` configuration file.
It has the following structure:

* `datafile`: name of the input file (the *preprocess* file). It is pre-filled with `project-name>_preproc.csv`;
* `output`: name of the output file. It is prefilled with `<project-name>_terms.csv`;
* `stdout`: if `true` the command also print the output to the standard output;
* `n-grams`: maximum size of an n-gram. All the n-gram with lengths from one word to this number of words are generated. By default, this field is filled with `4`;
* `min-frequency`: minimum number of occurrences of an n-gram. All the n-gram with less occurrences than this value are discarded. Pre-filled with `5`;
* `placeholder`: placeholder used to mark the barriers in the `preprocess` stage. All the n-grams containg this character or containing words that start and end with this character are discarded. It is prefilled with the character `@`;
* `column`: column of the input file with the text to elaborate. Pre-filled with `abstract_lem`;
* `delimiter`: field delimiter used by the input file. Pre-filled with `\t`.

### fawoc
The `fawoc` command runs `FAWOC` on a list produced by the previous commands.
This command accepts three sub-commands:

* `terms`: run `FAWOC` on the *terms* file;
* `journals`: run `FAWOC` on the *journals* file;
* `acronyms`: run `FAWOC` on the *acronyms* file.

Usage:

    python3 slrkit.py fawoc [--input LABEL] [--width WIDTH]

the optional arguments are passed to `FAWOC` and override the corresponding values in the configuration file:

* `--input`: label to review;
* `--width`: width of the `FAWOC` windows, in number of columns.

If the `fawoc` command is invoked without a sub-command, the `terms` sub-command is run.
Each sub-command writes to its own profiler file, in the `log` directory of the project.

#### fawoc terms

The `fawoc terms` sub-command allows the user to classify the *terms* file.
This command uses the `fawoc_terms.toml` configuration file that has the following structure:

* `datafile`: file to classify. Pre-filled with `<project-name>_terms.csv`;
* `input`: label to review;
* `dry-run`: if `true`, `FAWOC` don't write anything on the `datafile` on exit;
* `no-auto-save`: if `true`, no auto saving;
* `no-profile`: if `true`, no data is written to the profiler file;
* `width`: width of the `FAWOC` windows in columns.

The profiler file for this sub-command is `fawoc_terms_profiler.log` in the `log` directory of the project.

#### fawoc journals

The `fawoc journals` sub-command allows the user to classify the *journals* file.
This command uses the `fawoc_journals.toml` configuration file.
Its structure is the same of the `fawoc_terms.toml`.
The only difference is that the `datafile` field is pre-filled with `<project-name>_journals.csv`.

The profiler file for this sub-command is `fawoc_journals_profiler.log` in the `log` directory of the project.

#### fawoc acronyms

The `fawoc acronyms` sub-command allows the user to classify the *acronyms* file.
This command uses the `fawoc_acronyms.toml` configuration file.
Its structure is the same of the `fawoc_terms.toml`.
The only difference is that the `datafile` field is pre-filled with `<project-name>_acronyms.csv`.

The profiler file for this sub-command is `fawoc_acronyms_profiler.log` in the `log` directory of the project.

### lda
The `lda` command trains an LDA model and outputs the extracted topics and the association between topics and documents.

Usage:

    python3 slrkit.py lda [--config CONFIG]

The `--config` optional argument allows the user to specify a different configuration file than the default one.
This command runs the `lda.py` script.

The `lda` sub-command uses, by default, the `lda.toml` configuration file that has the following structure:

* `preproc_file`: name of the *preprocess* file. Pre-filled with `<project-name>_preproc.csv`;
* `terms_file`: name of the *terms* file. Pre-filled with `<project-name>_terms.csv`;
* `outdir`: path to the directory where to save the results. Pre-filled with the path to project directory;
* `text-column`: column of the *preprocess* file to elaborate. Pre-filled with `abstract_lem`;
* `title-column`: column in the *preprocess* file to use as document title. Pre-filled with `title`;
* `topics`: number of topic to extract. Pre-filled with `20`;
* `alpha`: alpha parameter of LDA. Pre-filled with `auto`;
* `beta`: beta parameter of LDA. Pre-filled with `auto`;
* `no_below`: keep tokens which are contained in at least this number of documents. Pre-filled with `20`;
* `no_above`: keep tokens which are contained in no more than this fraction of documents (fraction of total corpus size, not an absolute number). Pre-filled with `0.5`;
* `seed`: seed to be use in trainig;
* `model`: if `true` the lda model is saved to directory `<outdir>/lda_model`. The model is saved with name "model";
* `no-relevant`: if set, use only the term labelled as `keyword` in the *terms* file;
* `load-model`: path to a directory where a previously trained model is saved. Inside this directory the model named "model" is searched. the loaded model is used with the dataset file to generate the topics and the topic document association;
* `no_timestamp`: if `true`, no timestamp is added to the output file names;
* `placeholder`: placeholder for the barriers. Pre-filled with `@`;
* `delimiter`: field delimiter used in the *preprocess* file. Pre-filled with `\t`.

**IMPORTANT:**

there are some issues on the reproducibility of the LDA training.
Setting the `seed` value is not enough to guarantee the reproducibilty of the experiment.
It is also necessary to set the environment variable `PYTHONHASHSEED` to `0`.
The following command sets the variable for a single run in a Linux shell:

    PYTHONHASHSEED=0 python3 slrkit.py lda

Also using a saved model requires the use of the same seed used for training and the `PYTHONHASHSEED` to 0.
More information on the `PYTHONHASHSEED` variable can be found [here](https://docs.python.org/3/using/cmdline.html#envvar-PYTHONHASHSEED).

### report

The `report` command produces some reports with statistics about the paper analyzed by the `lda` command.
This command runs the `topic_report.py` script.

Usage:

    python3 slrkit.py report json_file

with `json_file` the path to the json file with the documents-topics association produced by the `lda` command (or the `lda.py` script).
This json file is usually called `lda_docs-topics_<date>_<time>.json`.

The command uses the `report.toml` configuration file that has the following structure:

* `ris_file`: name of the ris file of the bibliographical database. It is prefilled with `<project-name>.ris` and modified by the `import` command with the name of the file used as its input;
* `dir`: output directory where the templates and the reports are saved. If empty, the current directory is used;
* `minyear`: minimum year to consider. If empty, the minimum year found in the data is used;
* `maxyear`: maximum year to consider. If empty, the maximum year found in the data is used.

On the first run, the command copies the `report_template.md` and `report_template.tex` from the `report_template` directory inside this repository, to the current project.
These two files are used to create the reports.
The user can customize the two copied template as he wishes.

The command creates a directory named `report<timestamp>` containing:

* the report (called `report.md`) in markdown format;
* the report (called `report.tex`) in LaTeX format;
* a figure in png format (called `reportyear.png`) used by the two reports above;
* a directory `tables` with some LaTeX files used by the LaTeX report.

For information about the statistics reported, refer to the `topic_report.py` documentation in the [README](README.md) file.

### optimize_lda

The `optimize_lda` command runs the `lda_ga.py` script to find the best combination of parameters for an LDA model.

Usage:

    python3 slrkit.py optimize_lda

The `optimize_lda` sub-command uses the `optimize_lda.toml` configuration file that has the following structure:

* `preproc_file`: name of the *preprocess* file. Pre-filled with `<project-name>_preproc.csv`;
* `terms_file`: name of the *terms* file. Pre-filled with `<project-name>_terms.csv`;
* `ga_params`: path of the file with the parameters used by the GA. Pre-filled with the absolute path to the `optimize_lda_ga_params.toml` file in the configuration directory;
* `outdir`: path to the directory where to save the results. Pre-filled with the path to project directory;
* `text-column`: column of the *preprocess* file to elaborate. Pre-filled with `abstract_lem`;
* `title-column`: column in the *preprocess* file to use as document title. Pre-filled with `title`;
* `seed`: seed to be use in trainig;
* `placeholder`: placeholder for the barriers. Pre-filled with `@`;
* `delimiter`: field delimiter used in the *preprocess* file. Pre-filled with `\t`.
* `no_timestamp`: if `true`, no timestamp is added to the output file names;

The `ga_params` file has the following structure:

* `limits`: this section contains the ranges of the parameter;
  * `min_topics`: minimum number of topics;
  * `max_topics`: maximum number of topics;
  * `max_no_below`: maximum value of the no-below parameter. The minimum is always 1. A value of -1 means a tenth of the number of documents;
  * `min_no_above`: minimum value of the no-above parameter. The maximum is always 1.
* `algorithm`: this section contains the parameters used by the GA:
  * `mu`: number of individuals that will pass each generation;
  * `lambda`: number of individuals that are generated at each generation;
  * `initial`: size of the initial population;
  * `generations`: number of generation;
  * `tournament_size`: number of individuals randomly selected for the selection tournament.
* `probabilities`: this section contains the probabilities used by the script:
  * `mutate`: probability of mutation;
  * `component_mutation`: probability of mutation of each individual component;
  * `mate`: probability of crossover (also called mating);
  * `no_filter`: probability that a new individual is created with no term filter (no_above = no_below = 1);
* `mutate`: this section contains the parameters of the gaussian distributions used by the mutation for each parameter:
  * `topics.mu` and `topics.sigma` are the mean value and the standard deviation for the topics parameter;
  * `alpha_val.mu` and `alpha_val.sigma` are the mean value and the standard deviation for the value of the alpha parameter;
  * `beta.mu` and `beta.sigma` are the mean value and the standard deviation for the beta parameter;
  * `no_above.mu` and `no_above.sigma` are the mean value and the standard deviation for the no_above parameter;
  * `no_below.mu` and `no_below.sigma` are the mean value and the standard deviation for the no_below parameter;
  * `alpha_type.mu` and `alpha_type.sigma` are the mean value and the standard deviation for the type of the alpha parameter.

Refer to the documentation of the `lda_ga.py` script in [README.md](README.md) for more information about the behaviour of the script and the GA parameters.

The script outputs all the trained models in `<outdir>/<date>_<time>_lda_results/<UUID>`.
For each trained model is it produced a `toml` file with all the parameter already set to use the corresponding model with the `lda.py` script or the `lda` command.
These `toml` files are saved in `<outdir>/<date>_<time>_lda_results/<UUID>.toml`, and can be loaded in the `lda.py` script or the `lda` command using its `--config` option.
It also outputs a tsv file in `<outdir>/<date>_<time>_lda_results/results.csv` with the following format:

* `id`: progressive identification number;
* `topics`: number of topics;
* `alpha`: alpha value;
* `beta`: beta value;
* `no_below`: no-below value;
* `no_above`: no-above value;
* `coherence`: coherence score of the model;
* `times`: time spent evaluating this model;
* `seed`: seed used;
* `uuid`: UUID of the model;
* `num_docs`: number of document;
* `num_not_empty`: number of documents not empty after filtering.

The script, also outputs the extracted topics and the topics-documents aasociation produced by the best model.
The topics are output in `<outdir>/lda_terms-topics_<date>_<time>.json` and the topics assigned
to each document in `<outdir>/lda_docs-topics_<date>_<time>.json`.

**IMPORTANT:**

there are some issues on the reproducibility of the LDA training.
Setting the `seed` option (see below) is not enough to guarantee the reproducibilty of the experiment.
It is also necessary to set the environment variable `PYTHONHASHSEED` to `0`.
The following command sets the variable for a single run in a Linux shell:

    PYTHONHASHSEED=0 python3 slrkit.py optimize_lda

Also using a saved model requires the use of the same seed used for training and the `PYTHONHASHSEED` to 0.
More information on the `PYTHONHASHSEED` variable can be found [here](https://docs.python.org/3/using/cmdline.html#envvar-PYTHONHASHSEED).

### record
TODO

### lda_grid_search
TODO